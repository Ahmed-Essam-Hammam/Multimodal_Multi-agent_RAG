# ğŸ¤– Multimodal Multi-Agent RAG System

A production-ready **Retrieval-Augmented Generation (RAG)** system with **CLIP-powered multimodal search**, **multi-agent verification**, and **hallucination detection**. Built with LangGraph, Cerebras LLaMA 70B, and Streamlit.

<div align="center">

![Python](https://img.shields.io/badge/python-3.11-blue.svg)
![License](https://img.shields.io/badge/license-MIT-green.svg)
![Status](https://img.shields.io/badge/status-production-brightgreen.svg)

</div>

---

## âœ¨ Features

### ğŸ¯ Core Capabilities
- **Multi-Agent Architecture**: 3 specialized agents (Retrieval â†’ Verification â†’ Response)
- **Multimodal RAG**: Search across both text and images using CLIP embeddings
- **Visual Similarity Search**: Find diagrams by uploading query images
- **Hallucination Detection**: AI-powered verification with confidence scoring
- **Document Processing**: Automatic extraction of text and images from PDFs, DOCX, TXT

### ğŸ¨ CLIP Integration
- **Cross-Modal Search**: Text queries find visually relevant images
- **Image-to-Image Search**: Upload an image to find similar diagrams
- **Semantic Understanding**: Goes beyond keyword matching
- **True Visual Embeddings**: Processes actual image pixels, not descriptions

### ğŸ›¡ï¸ Safety & Quality
- **Verification Agent**: Validates responses against retrieved sources
- **Confidence Scoring**: 0-100% confidence for each answer
- **Source Citation**: Automatic attribution to source documents
- **Duplicate Filtering**: Removes inverted and duplicate images

### ğŸ–¥ï¸ Interfaces
- **Streamlit Web UI**: Beautiful, interactive chat interface
- **CLI**: Command-line interface for automation
- **Docker Support**: One-command deployment anywhere

---

## ğŸ—ï¸ Architecture

### Multi-Agent Pipeline

```
User Query
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  INPUT NODE                                     â”‚
â”‚  - Parse query                                  â”‚
â”‚  - Detect query type (text/image)               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  RETRIEVAL AGENT                                â”‚
â”‚  - CLIP text encoder â†’ query embedding          â”‚
â”‚  - Search vector DB (ChromaDB)                  â”‚
â”‚  - Retrieve: Top-K texts + images               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  VERIFICATION AGENT                             â”‚
â”‚  - Compare answer vs sources                    â”‚
â”‚  - Detect hallucinations                        â”‚
â”‚  - Calculate confidence score                   â”‚
â”‚  - Status: verified/unverified/hallucination    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  RESPONSE AGENT                                 â”‚
â”‚  - Generate answer (Cerebras LLaMA 70B)         â”‚
â”‚  - Include image references                     â”‚
â”‚  - Add source citations                         â”‚
â”‚  - Format final response                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
Final Answer + Images + Sources + Confidence
```

### Technology Stack

| Component | Technology |
|-----------|-----------|
| **Orchestration** | LangGraph |
| **LLM** | Cerebras LLaMA 70B |
| **Embeddings** | CLIP (ViT-Large) + Sentence Transformers |
| **Vector DB** | ChromaDB |
| **Document Processing** | PyMuPDF, python-docx |
| **Web UI** | Streamlit |
| **Containerization** | Docker |

---

## ğŸš€ Quick Start

### Prerequisites

- Python 3.11+
- Cerebras API Key ([Get one here](https://cerebras.ai/))
- Docker (optional, for containerized deployment)

### Option 1: Local Installation

```bash
# 1. Clone the repository
git clone https://github.com/yourusername/multimodal-rag.git
cd multimodal-rag

# 2. Create virtual environment
python -m venv myenv
myenv\Scripts\activate  # Windows
source myenv/bin/activate  # Mac/Linux

# 3. Install dependencies
pip install -r requirements.txt

# 4. Configure environment
cp .env.example .env
# Edit .env and add your CEREBRAS_API_KEY

# 5. Run Streamlit UI
streamlit run app_streamlit.py
```

Open browser: **http://localhost:8501**

### Option 2: Docker Deployment

```bash
# 1. Setup environment
cp .env.example .env
# Edit .env and add your API key

# 2. Run with Docker Compose
docker-compose up

# Or use the simple version (no build required)
docker-compose -f docker-compose.simple.yml up
```

Open browser: **http://localhost:8501**

---

## ğŸ“– Usage Guide

### Web Interface (Streamlit)

#### 1. Upload Documents
```
Documents Tab â†’ Upload PDF/DOCX â†’ Click "Ingest"
```

#### 2. Text Search
```
Chat Tab â†’ Type: "Explain transformer architecture" â†’ Send
```
Returns: Text answer + relevant images + sources

#### 3. Image Search
```
Image Search Tab â†’ Upload query image â†’ Click "Search Similar Images"
```
Returns: Visually similar images ranked by CLIP similarity

### Command Line Interface

#### Ingest Documents
```bash
python main.py --mode ingest --files "path/to/document.pdf"
```

#### Text Query
```bash
python main.py --mode query --query "What is attention mechanism?"
```

#### Image Search
```bash
python main.py --mode image_search --image "path/to/diagram.png"
```

#### Interactive Mode
```bash
python main.py --mode interactive
> transformer architecture
> image diagram.png
> exit
```

#### Reset Database
```bash
python main.py --mode reset
```

---

## ğŸ“ Project Structure

```
Multimodal_Multi-agent_RAG/
â”‚
â”œâ”€â”€ agents/                      # Multi-agent components
â”‚   â”œâ”€â”€ retrieval_agent.py      # Retrieves relevant content
â”‚   â”œâ”€â”€ verification_agent.py    # Validates responses
â”‚   â””â”€â”€ response_agent.py        # Generates final answer
â”‚
â”œâ”€â”€ utils/                       # Core utilities
â”‚   â”œâ”€â”€ document_processor.py   # Extract text + images from docs
â”‚   â”œâ”€â”€ vector_store.py         # ChromaDB vector database
â”‚   â”œâ”€â”€ multimodal_embeddings.py # CLIP embeddings
â”‚   â””â”€â”€ image_search.py         # Visual similarity search
â”‚
â”œâ”€â”€ tools/                       # Agent tools
â”‚   â””â”€â”€ retrieval_tools.py      # Search and fetch tools
â”‚
â”œâ”€â”€ data/                        # Data directory
â”‚   â”œâ”€â”€ uploads/                # Uploaded documents
â”‚   â”‚   â””â”€â”€ extracted_images/   # Extracted diagrams
â”‚   â””â”€â”€ chroma_db/              # Vector database
â”‚
â”œâ”€â”€ graph.py                     # LangGraph workflow
â”œâ”€â”€ nodes.py                     # Graph node definitions
â”œâ”€â”€ state.py                     # State management
â”œâ”€â”€ config.py                    # Configuration
â”œâ”€â”€ main.py                      # CLI entrypoint
â”œâ”€â”€ app_streamlit.py            # Streamlit web UI
â”‚
â”œâ”€â”€ Dockerfile                   # Docker image definition
â”œâ”€â”€ docker-compose.yml          # Multi-container orchestration
â”œâ”€â”€ requirements.txt            # Python dependencies
â””â”€â”€ .env.example                # Environment template
```

---

## âš™ï¸ Configuration

### Environment Variables (`.env`)

```bash
# Required
CEREBRAS_API_KEY=your_api_key_here

# Model Settings
MODEL_NAME=llama-3.3-70b
TEMPERATURE=0.0
MAX_TOKENS=1000

# Retrieval Settings
TOP_K_RESULTS=5
SIMILARITY_THRESHOLD=0.3  

# Paths
CHROMA_DB_PATH=./data/chroma_db
UPLOAD_DIR=./data/uploads

# Document Processing
CHUNK_SIZE=1000
CHUNK_OVERLAP=200
MAX_IMAGE_SIZE=800,800
```

### Model Selection

**CLIP Models** (in `utils/multimodal_embeddings.py`):
- `openai/clip-vit-base-patch32` - Fast, 150MB (recommended)
- `openai/clip-vit-large-patch14` - Accurate, 500MB (slower)

**LLM Models**:
- `llama3.1-70b` - Best quality (default)
- `llama3.1-8b` - Faster, lower cost

---

## ğŸ¯ Key Features Explained

### 1. CLIP Multimodal Search

**Problem:** Traditional RAG only matches keywords. "transformer architecture" won't find diagrams labeled "encoder-decoder model".

**Solution:** CLIP creates a shared embedding space:
```python
Text: "transformer architecture" â†’ [0.23, 0.87, -0.45, ...]
Image: [actual diagram pixels]  â†’ [0.21, 0.89, -0.43, ...]
Similarity: 0.94 âœ… (They represent the same thing!)
```

**Result:** Text queries find semantically similar images, even with different labels!

### 2. Multi-Agent Verification

**Problem:** LLMs hallucinate - they make up facts not in the sources.

**Solution:** 3-agent pipeline:
1. **Retrieval Agent**: Finds relevant content
2. **Verification Agent**: Checks if answer is supported by sources
3. **Response Agent**: Generates verified answer with citations

**Result:** Confidence scores and hallucination warnings!

### 3. Image Deduplication

**Problem:** PDFs contain duplicate images (normal + inverted versions).

**Solution:** Intelligent filtering:
- Skip dark images (mean brightness < 50)
- Skip blank images (mean brightness > 250)  
- Skip duplicates (image similarity)
- Skip tiny images (< 100x100)

**Result:** Only clean, unique, readable images!

---

## ğŸ”¬ Advanced Usage

### Custom Embedding Model

Edit `utils/vector_store.py`:
```python
def _initialize_embeddings(self):
    clip = MultimodalEmbeddings(model_name="openai/clip-vit-base-patch32")
    # Change to: "openai/clip-vit-large-patch14" for higher accuracy
```

### Adjust Similarity Threshold

Lower threshold = more results (less strict):
```python
# In .env
SIMILARITY_THRESHOLD=0.1  # Permissive (recommended for CLIP)
SIMILARITY_THRESHOLD=0.3  # Balanced
SIMILARITY_THRESHOLD=0.5  # Strict
```

### Batch Document Ingestion

```python
from main import MultimodalRAGSystem

system = MultimodalRAGSystem()
documents = ["doc1.pdf", "doc2.pdf", "doc3.pdf"]
system.ingest_documents(documents)
```

---

## ğŸ› Troubleshooting

### No Images Retrieved

**Symptom:** `Retrieved 5 texts and 0 images`

**Solution:**
```bash
# 1. Check threshold
# Lower SIMILARITY_THRESHOLD in .env to 0.1

# 2. Verify CLIP embeddings
python -c "from utils.multimodal_embeddings import MultimodalEmbeddings; print('âœ“ CLIP OK')"

# 3. Re-ingest documents
python main.py --mode reset
python main.py --mode ingest --files "your_doc.pdf"
```

### Docker Build Fails

**Symptom:** `error writing manifest blob`

**Solution:**
```bash
# Use simple compose (no build)
docker-compose -f docker-compose.simple.yml up

# Or increase Docker resources:
# Docker Desktop â†’ Settings â†’ Resources â†’ Disk: 100GB, Memory: 8GB
```

### CLIP Loading Slow

**First run:** CLIP downloads 500MB model (one-time)

**Speed up:** Use smaller model:
```python
# In utils/multimodal_embeddings.py
MultimodalEmbeddings(model_name="openai/clip-vit-base-patch32")  # 150MB
```

---

## ğŸ“Š Performance

### Benchmarks (on CPU)

| Operation | Time | Notes |
|-----------|------|-------|
| **Document Ingestion** | ~30s per 10-page PDF | Includes image extraction + embedding |
| **Text Query** | 2-5s | Retrieval (0.5s) + LLM (2-4s) |
| **Image Search** | 1-2s | CLIP embedding + similarity search |
| **CLIP Model Load** | 10-20s | One-time on startup |

### Resource Requirements

| Component | Minimum | Recommended |
|-----------|---------|-------------|
| **RAM** | 4 GB | 8 GB |
| **Disk** | 5 GB | 20 GB |
| **Python** | 3.11+ | 3.11 |
| **GPU** | Not required | Optional (10x faster CLIP) |

---

## ğŸ” Security Best Practices

1. **Never commit `.env`** - Contains API keys
2. **Add to `.gitignore`:**
   ```
   .env
   data/
   myenv/
   __pycache__/
   ```
3. **Use environment-specific configs:**
   - `.env.development`
   - `.env.production`
4. **Rotate API keys** regularly
5. **Limit file upload sizes** in production

---

## ğŸ¤ Contributing

We welcome contributions! Here's how:

1. Fork the repository
2. Create a feature branch: `git checkout -b feature/amazing-feature`
3. Commit changes: `git commit -m 'Add amazing feature'`
4. Push to branch: `git push origin feature/amazing-feature`
5. Open a Pull Request

### Development Setup

```bash
# Install development dependencies
pip install -r requirements.txt
pip install pytest black flake8

# Run tests
pytest

# Format code
black .

# Lint
flake8 .
```

---

## ğŸ“ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

---

## ğŸ™ Acknowledgments

- **LangGraph** - Multi-agent orchestration framework
- **Cerebras** - Fast LLM inference
- **OpenAI CLIP** - Multimodal embeddings
- **ChromaDB** - Vector database
- **Streamlit** - Web interface framework

---

## ğŸ“ Support

- **Issues**: [GitHub Issues](https://github.com/yourusername/multimodal-rag/issues)
- **Discussions**: [GitHub Discussions](https://github.com/yourusername/multimodal-rag/discussions)
- **Email**: your.email@example.com

---

## ğŸ—ºï¸ Roadmap

- [ ] GPU acceleration for CLIP
- [ ] Support for more document formats (PPT, Excel)
- [ ] Multi-language support
- [ ] API endpoint (FastAPI)
- [ ] Ollama integration (local LLMs)
- [ ] Vector database alternatives (Pinecone, Weaviate)
- [ ] Advanced visualization of retrieval results
- [ ] Batch processing API

---

## ğŸ“ˆ Changelog

### v1.0.0 (2026-02-14)
- âœ¨ Initial release
- ğŸ¨ CLIP multimodal search
- ğŸ¤– Multi-agent verification
- ğŸ–¼ï¸ Image-to-image search
- ğŸ³ Docker support
- ğŸ“± Streamlit web UI

---

<div align="center">

**â­ Star this repo if you find it useful!**

Built with â¤ï¸ using LangGraph, CLIP, and Cerebras

</div>
